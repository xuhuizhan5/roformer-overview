{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roformer code demostration\n",
    "\n",
    "> Author: Daniel Zhan\n",
    "\n",
    "> Email: xuhui.zhan@vanderbilt.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demostration of perform rotating on token embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.53273496,  0.24812483,  0.44309287, -0.38605851,  0.4903044 ,\n",
       "        -0.32399702],\n",
       "       [ 0.13329626, -2.25115923,  0.49586427,  2.19991009,  0.20024322,\n",
       "         2.24619421],\n",
       "       [-1.89775242,  2.88864878,  1.02277696, -3.30146684,  1.45086962,\n",
       "        -3.13699432],\n",
       "       [ 1.99119877,  1.39832662, -2.30075392, -0.79165729, -2.17568955,\n",
       "        -1.08929558]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_rotary_matrix(dim, pos):\n",
    "    \"\"\"\n",
    "    Calculate the rotary position embedding (RoPE) matrix for a given position.\n",
    "\n",
    "    Args:\n",
    "    - dim (int): The dimensionality of the model (should be even).\n",
    "    - pos (int): The position for which to calculate the RoPE matrix.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: The RoPE matrix for the given position and dimension.\n",
    "    \"\"\"\n",
    "    # Ensure the dimension is even since we're dealing with 2D rotations\n",
    "    if dim % 2 != 0:\n",
    "        raise ValueError(\"The dimension should be an even number.\")\n",
    "\n",
    "    # Frequency of the rotation for each dimension pair\n",
    "    freqs = np.arange(dim // 2) / (dim // 2)\n",
    "    inv_freqs = 1 / (10000 ** freqs)\n",
    "    \n",
    "    # Calculate the angles for the rotation\n",
    "    angles = pos * inv_freqs\n",
    "    sin_angles = np.sin(angles)\n",
    "    cos_angles = np.cos(angles)\n",
    "    \n",
    "    # Construct the rotation matrix\n",
    "    rotation_matrix = np.empty((dim, dim))\n",
    "    rotation_matrix[0::2, 0::2] = cos_angles\n",
    "    rotation_matrix[1::2, 1::2] = cos_angles\n",
    "    rotation_matrix[0::2, 1::2] = -sin_angles\n",
    "    rotation_matrix[1::2, 0::2] = sin_angles\n",
    "    \n",
    "    return rotation_matrix\n",
    "\n",
    "def apply_rotary_embedding(embedding_matrix, pos):\n",
    "    \"\"\"\n",
    "    Apply the rotary position embedding to the token embedding matrix.\n",
    "\n",
    "    Args:\n",
    "    - embedding_matrix (np.array): The token embedding matrix.\n",
    "    - pos (int): The position in the sequence.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: The rotated token embedding matrix.\n",
    "    \"\"\"\n",
    "    dim = embedding_matrix.shape[1]\n",
    "    rope_matrix = get_rotary_matrix(dim, pos)\n",
    "    \n",
    "    # Apply the rotation to each token embedding\n",
    "    rotated_embedding_matrix = embedding_matrix @ rope_matrix\n",
    "    return rotated_embedding_matrix\n",
    "\n",
    "# Example usage:\n",
    "# Suppose we have a token embedding matrix of size (num_tokens, dim_model)\n",
    "# where num_tokens is the number of tokens in the sequence and dim_model is the model dimension.\n",
    "# Let's define a dummy embedding matrix and a position 'p' for which we want to calculate the rotation.\n",
    "\n",
    "num_tokens = 4  # Just an example, typically the number of tokens in your sequence\n",
    "dim_model = 6   # The dimension of the model, should be even\n",
    "p = 3           # The position in the sequence for which to calculate the rotary embeddings\n",
    "\n",
    "# Create a dummy token embedding matrix\n",
    "embedding_matrix = np.random.randn(num_tokens, dim_model)\n",
    "\n",
    "# Apply rotary embeddings\n",
    "rotated_embedding_matrix = apply_rotary_embedding(embedding_matrix, p)\n",
    "rotated_embedding_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roformer config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RoFormerModel, RoFormerConfig\n",
    "\n",
    "# Initializing a RoFormer junnyu/roformer_chinese_base style configuration\n",
    "configuration = RoFormerConfig()\n",
    "\n",
    "# Initializing a model (with random weights) from the junnyu/roformer_chinese_base style configuration\n",
    "model = RoFormerModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoFormerTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今', '天', '天', '气', '非常', '好', '。']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RoFormerTokenizer\n",
    "\n",
    "tokenizer = RoFormerTokenizer.from_pretrained(\"junnyu/roformer_chinese_base\")\n",
    "tokenizer.tokenize(\"今天天气非常好。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoFormerTokenizerFast\n",
    "Construct a “fast” RoFormer tokenizer (backed by HuggingFace’s tokenizers library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今', '天', '天', '气', '非常', '好', '。']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RoFormerTokenizerFast\n",
    "\n",
    "tokenizer = RoFormerTokenizerFast.from_pretrained(\"junnyu/roformer_chinese_base\")\n",
    "tokenizer.tokenize(\"今天天气非常好。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoFormerForSequenceClassification\n",
    "RoFormer Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.\n",
    "\n",
    "This model is a PyTorch torch.nn.Module sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example for single label classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RoFormerForSequenceClassification were not initialized from the model checkpoint at junnyu/roformer_chinese_base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'roformer.encoder.embed_positions.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: LABEL_0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, RoFormerForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"junnyu/roformer_chinese_base\")\n",
    "model = RoFormerForSequenceClassification.from_pretrained(\"junnyu/roformer_chinese_base\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_id])\n",
    "\n",
    "# # To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "# num_labels = len(model.config.id2label)\n",
    "# model = RoFormerForSequenceClassification.from_pretrained(\"junnyu/roformer_chinese_base\", num_labels=num_labels)\n",
    "\n",
    "# labels = torch.tensor([1])\n",
    "# loss = model(**inputs, labels=labels).loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example for multi label classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RoFormerForSequenceClassification were not initialized from the model checkpoint at junnyu/roformer_chinese_base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'roformer.encoder.embed_positions.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RoFormerForSequenceClassification were not initialized from the model checkpoint at junnyu/roformer_chinese_base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'roformer.encoder.embed_positions.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, RoFormerForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"junnyu/roformer_chinese_base\")\n",
    "model = RoFormerForSequenceClassification.from_pretrained(\"junnyu/roformer_chinese_base\", problem_type=\"multi_label_classification\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class_ids = torch.arange(0, logits.shape[-1])[torch.sigmoid(logits).squeeze(dim=0) > 0.5]\n",
    "\n",
    "# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\n",
    "num_labels = len(model.config.id2label)\n",
    "model = RoFormerForSequenceClassification.from_pretrained(\n",
    "    \"junnyu/roformer_chinese_base\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "labels = torch.sum(\n",
    "    torch.nn.functional.one_hot(predicted_class_ids[None, :].clone(), num_classes=num_labels), dim=1\n",
    ").to(torch.float)\n",
    "loss = model(**inputs, labels=labels).loss\n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoFormerForTokenClassification\n",
    "RoFormer Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n",
    "\n",
    "This model is a PyTorch torch.nn.Module sub-class. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RoFormerForTokenClassification were not initialized from the model checkpoint at junnyu/roformer_chinese_base and are newly initialized: ['roformer.encoder.embed_positions.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, RoFormerForTokenClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"junnyu/roformer_chinese_base\")\n",
    "model = RoFormerForTokenClassification.from_pretrained(\"junnyu/roformer_chinese_base\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"HuggingFace is a company based in Paris and New York\", add_special_tokens=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_token_class_ids = logits.argmax(-1)\n",
    "\n",
    "# Note that tokens are classified rather then input words which means that\n",
    "# there might be more predicted token classes than words.\n",
    "# Multiple token classes might account for the same word\n",
    "predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "\n",
    "labels = predicted_token_class_ids\n",
    "loss = model(**inputs, labels=labels).loss\n",
    "display(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
